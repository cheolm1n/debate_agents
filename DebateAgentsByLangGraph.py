import streamlit as st
from dotenv import load_dotenv
from langchain.schema import SystemMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage

load_dotenv()

# Streamlit ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
st.set_page_config(page_title="Ïù¥Í∏∞Îäî Ìé∏ Ïö∞Î¶¨ Ìé∏", layout="wide")

# Í≤ÄÏÉâ ÎèÑÍµ¨ Ï¥àÍ∏∞Ìôî
search_tool = TavilySearchResults(k=5)
names_search = {"Pro(Ï∞¨ÏÑ±)": [search_tool], "Con(Î∞òÎåÄ)": [search_tool]}

# ÌÜ†Î°† Ï£ºÏ†ú ÏûÖÎ†•
topic = st.chat_input("ÌÜ†Î°† Ï£ºÏ†úÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
word_limit = 50

agent_descriptions = {}
pros_agent = None
cons_agent = None


# ÌÜ†Î°† Ï∞∏Í∞ÄÏûêÏóê ÎåÄÌïú ÏÑ§Î™Ö ÏÉùÏÑ±
def generate_agent_description(name, topic):
    agent_specifier_prompt = [
        SystemMessage(content="You can add detail to the description of the conversation participant."),
        HumanMessage(
            content=f"Here is the topic of conversation: {topic}. Please reply with a description of {name}, in {word_limit} words or less in expert tone. Speak directly to {name}. Answer in KOREAN.")
    ]
    return ChatOpenAI(temperature=0)(agent_specifier_prompt).content


# ÌÜ†Î°† Ï∞∏Í∞ÄÏûê Ï†ïÎ≥¥Î•º ÌÜµÌï¥ Ï£ºÏ†úÎ•º Îçî Íµ¨Ï≤¥Ìôî
def specify_topic(topic, agent_descriptions):
    prompt = [
        SystemMessage(content="You can make a topic more specific."),
        HumanMessage(content=f"{topic}\nPlease make the topic more specific. Consider the participants: {agent_descriptions}. Answer in Korean.")
    ]
    return ChatOpenAI(temperature=1.0)(prompt).content


# Í∞Å ÌÜ†Î°†Ïûê ÏóêÏù¥Ï†ÑÌä∏Ïùò ÏãúÏä§ÌÖú Î©îÏãúÏßÄ ÏÉùÏÑ±
# def generate_system_message(name, description, tools):
#     return f"""Here is the topic of conversation: {topic}
# Your name is {name}.
# Your description is as follows: {description}
# Your goal is to persuade your conversation partner of your point of view.
# DO look up information with your tool to refute your partner's claims.
# DO cite your sources.
# DO NOT fabricate fake citations.
# DO NOT cite any source that you did not look up.
# DO NOT restate something that has already been said in the past.
# DO NOT add anything else.
# DO NOT speak from the perspective of other participants.
# Stop speaking the moment you finish speaking from your perspective.
# Answer in KOREAN."""


# Îã§Ïùå Î∞úÏñ∏ÏûêÎ•º ÏÑ†ÌÉùÌïòÎäî Ìï®Ïàò
# def select_next_speaker(step: int, agents: List[DialogueAgentWithTools]) -> int:
#     return step % len(agents)


# ÏãúÎÆ¨Î†àÏù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÌÜ†Î°† ÏãúÏûë
# simulator = DialogueSimulator(agents=agents, selection_function=select_next_speaker)
# simulator.reset()
# simulator.inject("Moderator", specified_topic)

# Streamlit Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
st.markdown("# AI Vs AI ü•ä")
st.markdown("AIÏóêÍ≤å ÌéòÎ•¥ÏÜåÎÇòÎ•º Î∂ÄÏó¨ÌïòÏó¨ AgentÎ°ú Ïù∏ÌÑ∞ÎÑ∑ Í≤ÄÏÉâ ToolÏùÑ Ï£ºÏñ¥ ÌÜ†Î°†ÌïòÍ≤å ÌïòÏòÄÏäµÎãàÎã§.")
st.markdown("ÌÜ†Î°† Ï£ºÏ†úÎäî Ï∞¨ÏÑ±Í≥º Î∞òÎåÄÎ°ú ÎÇòÎâòÏñ¥ ÌÜ†Î°†Ìï† Ïàò ÏûàÎäî Ï£ºÏ†úÎ°ú ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî!")

# ÏÇ¨Ïö©ÏûêÏóêÍ≤å Ï£ºÏ†ú Ï∂úÎ†•
# if topic:
#     with st.chat_message("user", avatar="üßë"):
#         st.write(topic)
#
#     with st.chat_message("assistant", avatar="ü§ñ"):
#         st.write(specified_topic)
#
#     # ÏµúÎåÄ Î∞òÎ≥µ ÌöüÏàòÎßåÌÅº ÌÜ†Î°†ÏùÑ ÏßÑÌñâ
#     for n in range(6):
#         name, message = simulator.step()
#         with st.chat_message("assistant", avatar={"Pro(Ï∞¨ÏÑ±)": "üôÜ‚Äç‚ôÇÔ∏è", "Con(Î∞òÎåÄ)": "üôÖ‚Äç‚ôÇÔ∏è"}[name]):
#             st.write(message)

#

# Í∞Å Ï∞∏Í∞ÄÏûêÏóê ÎåÄÌïú ÏÑ§Î™Ö ÏÉùÏÑ±
# agent_descriptions = {name: generate_agent_description(name, topic) for name in names_search}
#
# specified_topic = specify_topic(topic, agent_descriptions)

# agent_system_messages = {
#     name: generate_system_message(name, description, tools)
#     for (name, tools), description in zip(names_search.items(), agent_descriptions.values())
# }
# Í∞Å ÌÜ†Î°†Ïûê ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
# agents = [
#     DialogueAgentWithTools(
#         name=name,
#         system_message=SystemMessage(content=system_message),
#         model=ChatOpenAI(model_name="gpt-4-turbo-preview", temperature=0.2),
#         tools=tools
#     )
#     for (name, tools), system_message in zip(names_search.items(), agent_system_messages.values())
# ]

# LangGraph
import operator
from typing import Annotated, Sequence
from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    sender: str
    topic: str


from langchain_core.messages import (
    FunctionMessage,
    HumanMessage,
)

from langchain.tools.render import format_tool_to_openai_function
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import END, StateGraph


def create_agent(llm, tools, system_message: str):
    # ÏóêÏù¥Ï†ÑÌä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    functions = [format_tool_to_openai_function(t) for t in tools]

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful AI assistant, collaborating with other assistants."
                " Use the provided tools to progress towards answering the question."
                " If you are unable to fully answer, that's OK, another assistant with different tools "
                " will help where you left off. Execute what you can to make progress."
                " If you or any of the other assistants have the final answer or deliverable,"
                " prefix your response with FINAL ANSWER so the team knows to stop."
                " You have access to the following tools: {tool_names}.\n{system_message}",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt.partial(system_message=system_message)
    prompt = prompt.partial(tool_names=", ".join(
        [tool.name for tool in tools]))
    return prompt | llm.bind_functions(functions)


import functools


def agent_node(state, agent, name):
    result = agent.invoke(state)

    st.write(result)

    if isinstance(result, FunctionMessage):
        pass
    else:
        result = HumanMessage(**result.dict(exclude={"type", "name"}), name=name)
    return {
        "messages": [result],
        "sender": name,
    }


def topic_agent_node(state, name):
    agent_descriptions = {name: generate_agent_description(name, topic) for name in names_search}
    st.write(agent_descriptions)
    state["topic"] = specify_topic(topic, agent_descriptions)
    st.write(state["topic"])
    state["sender"] = name
    pros_agent = create_agent(
        llm,
        [search_tool],
        system_message=agent_descriptions["Pros(Ï∞¨ÏÑ±)"],
    )
    cons_agent = create_agent(
        llm,
        [search_tool],
        system_message=agent_descriptions["Cons(Ï∞¨ÏÑ±)"],
    )
    return state


# def moderator_agent_node(state, name):
#
#
#     return state

llm = ChatOpenAI(model="gpt-4-1106-preview")

# Pros/Cons agent and node
topic_agent = create_agent(llm, [search_tool],
                           system_message=f"You can make a topic more specific. {topic}\nPlease make the topic more specific. Consider the participants: {agent_descriptions}. Answer in Korean.")
topic_node = functools.partial(topic_agent_node, agent=topic_agent, name="topic")

# moderator_agent = create_agent(llm, [search_tool],
#                                system_message=f"You can make a topic more specific. {topic}\nPlease make the topic more specific. Consider the participants: {agent_descriptions}. Answer in Korean.")
# moderator_node = functools.partial(agent_node, agent=moderator_agent, name="moderator")


pros_node = functools.partial(agent_node, agent=pros_agent, name="pros")

cons_node = functools.partial(agent_node, agent=cons_agent, name="cons")


def router(state):
    # ÏÉÅÌÉú Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú Îã§Ïùå Îã®Í≥ÑÎ•º Í≤∞Ï†ïÌïòÎäî ÎùºÏö∞ÌÑ∞ Ìï®Ïàò
    now = state["turn"]

    if now > 6:
        return "end"
    elif now % 2 != 0:
        return "pros"
    else:
        return "cons"


workflow = StateGraph(AgentState)

workflow.add_node("Topic", topic_node)
workflow.add_node("Pros", pros_node)
workflow.add_node("Cons", cons_node)
# workflow.add_node("Moderator", moderator_node)

workflow.add_edge("Topic", "Pros")
# workflow.add_edge("Topic", "Moderator")
# workflow.add_edge("Moderator", "Pros")

workflow.add_conditional_edges(
    "Pros",
    router,
    {"pros": "Pros", "cons": "Cons", "end": END},
)

workflow.add_conditional_edges(
    "Cons",
    router,
    {"pros": "Pros", "cons": "Cons", "end": END},
)

workflow.set_entry_point("Topic")
graph = workflow.compile()

if topic:
    # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï∂îÍ∞Ä
    # st.session_state.messages.append({"role": "user", "content": topic, "avatar": "üßë"})
    with st.chat_message("user", avatar="üßë"):
        st.write(topic)

    graph.invoke() ## how?

